{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Linear Regression Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oBYKM2lf-cq"
   },
   "outputs": [],
   "source": [
    "## Your code here (click on the window and type 'b' if you want to split in more than one code window)\n",
    "\n",
    "# Step 1: Read data, report missing data\n",
    "\n",
    "import pandas as pd #importing pandas\n",
    "train = pd.read_csv('C:/Users/sssalas/OneDrive - Philippine Competition Commission/Desktop/project1/real-estate-valuation-with-linear-models/Regression_Supervised_Train.csv') \n",
    "#reading train dataset\n",
    "test = pd.read_csv('C:/Users/sssalas/OneDrive - Philippine Competition Commission/Desktop/project1/real-estate-valuation-with-linear-models/Regression_Supervised_Test.csv')  \n",
    "#reading test dataset\n",
    "\n",
    "datasets = [train, test]\n",
    "\n",
    "for i in datasets:\n",
    "    print(i.shape)\n",
    "# there are two more columns in the train dataset, 'parcelvalue' and 'mypointer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of data for later use\n",
    "train_original=train.copy()\n",
    "test_original=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['totaltaxvalue', 'buildvalue', 'landvalue', 'mypointer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the columns suggested to be deleted in the instructions to avoid high correlation\n",
    "train = train.drop(to_drop, axis=1)\n",
    "\n",
    "test = test.drop(to_drop[:3], axis=1)\n",
    "#because we dropped these value columns, it would be quite difficult to get a low mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train, test]\n",
    "\n",
    "for i in datasets:\n",
    "    print(i.shape)\n",
    "# now, the two datasets only differ in features in terms of parcel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting observations per feature in the train and test dataset with null values\n",
    "for i in datasets:\n",
    "    print(i.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sb8-B91zf-cq"
   },
   "outputs": [],
   "source": [
    "# Step 2: Remove features with missing data, and then observations with missing data\n",
    "train = train.dropna(axis=1, thresh=round(0.60*len(train.index)))\n",
    "#any feature with  more than 40% missing values will be removed. in other words, we keep only the columns whose 60% of the values are not NaNs\n",
    "\n",
    "train.shape #checking the dimensions, now we only have 19 features as opposed to 44 before. the number of our observations is 24755."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = train.columns #we store the column names that have been left in the train dataset. this will be our index for the columns that we will retain for the test dataset\n",
    "train_columns = train_columns.drop([\"parcelvalue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[train_columns]\n",
    "# now, we only retain columns in the test dataset that are present in the train dataset as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape\n",
    "# we confirm that there are only 18 features left in our test dataset. the train dataset still has more than 1 column (parcelvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train, test]\n",
    "\n",
    "for i in datasets:\n",
    "    print(i.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(train) #we notice that there are still observations in train that have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna() #we drop observations that have missing values in the train dataset, \n",
    "#not necessary to put axis=0 because that is the default\n",
    "\n",
    "train.shape #now the dimensions of our train dataset is (12560, 19). The number of our observations is 12560. This is about half of the number of observations before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.dropna() #we drop observations that have missing values in the train dataset\n",
    "\n",
    "\n",
    "test.shape #now the dimensions of our train dataset is (2746, 18).\n",
    "# we still are left with the same number of obs and columns because there are no missing values in the rows of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train, test]\n",
    "\n",
    "for i in datasets:\n",
    "    print(i.isnull().sum())\n",
    "\n",
    "# now, we can confirm that there are no more null values in our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we notice that there are two countycodes, therefore we check if they are one and the same using corr. if there is perfect correlation, we drop one of them\n",
    "corr_county = train['countycode'].corr(train['countycode2'])\n",
    "print(corr_county) #corr_county = -1.0, therefore there is perfect correlation. \n",
    "#therefore we drop county2 in both datasets. note that this is only the purpose of this model. \n",
    "#it might be the case that the neighborhood codes hold other informations that are not just categorical (e.g. proximity)\n",
    "\n",
    "#we also drop lotid because it has no cardinal or ordinal meaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have also told to dropped the lotid because this name is arbitararily set and wouldn't help us with our pracelvalue predictions\n",
    "\n",
    "train = train.drop(['countycode2', 'lotid'], axis=1) #now we only have 17 columns in train dataset\n",
    "test = test.drop(['countycode2', 'lotid'], axis=1) #now we only have 16 columns in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe() #we check some summary statistics of our train dataset\n",
    "#at the same time we get a clear view of the columns that are left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe() #we check some summary statistics of our test dataset\n",
    "#at the same time we get a clear view of the columns that are left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "#parcelvalue is the only difference in column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw8otDpef-cq"
   },
   "outputs": [],
   "source": [
    "y_train = train.loc[:,'parcelvalue'] #we isolate the dependent variables from the whole training dataset\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#histogram\n",
    "\n",
    "sns.displot(pd.Series(y_train))\n",
    "#we see that the distribution of parcelvalue is highly positively skewed, so we might want to trasnform this to log\n",
    "#this will change the interpretation of the coefficients\n",
    "#but since we are doing machine learning, this step is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['parcelvalue'], axis=1) #we create our the regressors training dataset by creating a dataframe that contains all elements of the train dataset execpt parcelvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape # dimensions (12560, 570), one less than the train dataset itself because we do not have 'parcelvalue' here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#same dimensions as X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I will be repeating steps 4 to 8 multiple times since I will have different models, transformations of features, and parameter optimizations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression using non-transformed features**\n",
    "- we first try to fit our model based on non-transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build your model and get predictions from train data\n",
    "from sklearn.linear_model import LinearRegression #import the Linear Regression \n",
    "regr = LinearRegression() #store the function to an object\n",
    "\n",
    "regr.fit(X_train,y_train) #we fit the model\n",
    "y_hat_train = regr.predict(X_train) #we produce predictions from our fitted model based on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat_train)\n",
    "y_hat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Assess expected accuracy\n",
    "\n",
    "#we first assess the expected accurary of the model we have fitted using the original train and test datasets (the one without polynomial features and dummies)\n",
    "##in-sample\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "plt.figure() #creating a blank plot\n",
    "plt.scatter(x=y_train,y=y_hat_train) #plotting the points\n",
    "plt.plot(y_train,y_train,c=\"red\") #plotting a 45 degree line\n",
    "r_squared = r2_score(y_train, y_hat_train) #i use r_2 score instead of what Jack uses in class (getting correlation then squaring) because this is cleaner and more straightforward\n",
    "plt.title('R-squared equals %.3f' %r_squared) \n",
    "\n",
    "# the value of R-squared is 0.485, there is some correlation between the y_train but not strong\n",
    "#note that the interpretation of the red line is this: if all points lie on the red line, then there is perfect correlation\n",
    "# we see that this is true for lower values of y_train but not for higher ones\n",
    "#we can say that higher values are not predicted well by our model.\n",
    "#therefore later on we need to optimize paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I keep on using this kind of plot, so I will create a function\n",
    "# I have already imported this custom function in the beginning\n",
    "\n",
    "#def my_r2_plot(y_train, y_hat_train):\n",
    "    #plt.figure() \n",
    "    #plt.scatter(x=y_train,y=y_hat_train) \n",
    "    #plt.plot(y_train,y_train,c=\"red\") \n",
    "    #r_squared = r2_score(y_train, y_hat_train)\n",
    "    #plt.title('R-squared equals %.3f' %r_squared) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU0jANEHf-cq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##producing cross-validated predictions\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "y_hat_cv = cvp(regr, X_train, y_train, cv=80) # #try first cv=80 then leave-one-out CV when cv=12560      \n",
    "                                  # and 12560 because n=12560 #leave-one-out taking so long so i'll put 50 first (100 k-folds)\n",
    "\n",
    "#If a value for k is chosen that does not evenly split the data sample, \n",
    "#then one group will contain a remainder of the examples. \n",
    "#It is preferable to split the data sample into k groups with the same number of samples, \n",
    "#such that the sample of model skill scores are all equivalent. \n",
    "#from https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\n",
    "#I'll use 80 since it's a factor of 12560\n",
    "\n",
    "my_r2_plot(y_train, y_hat_cv)\n",
    "\n",
    "# the value of R-squared is 0.480, there is some correlation between the y_train but not strong\n",
    "#therefore go back to previous steps and optimize paramaters\n",
    "\n",
    "#also, note that the r-squared with cross-validated predictions is lower than the usual in-sample r-squared (0.480 < 0.485).\n",
    "# in other words, the performance of the model decreased, but not drastically\n",
    "\n",
    "#this is as expected since the fitted model will be better to produce predictions based on the data it has seen before than on the data it has never since\n",
    "#but also note that the r-squares of the in-sample and out-sample are not that far. therefore we could say that there is no overfitting (the model is bad both on preicting in-sample and out-sample data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we could also use cross_val_score to assess expect accuracy \n",
    "\n",
    "#from sklearn.model_selection import cross_val_score as cvs\n",
    "\n",
    "#accuracy = cvs(regr, X_train, y_train, scoring='accuracy', cv = 10)\n",
    "#print(accuracy)\n",
    "\n",
    "##but for some reason, the above code is not working as expected,\n",
    "#checking stack exchange, the code may not be working because of the version of python that I'm using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We produce predicted values using leave-one-out cv just to see if it would be very different from cv=80.\n",
    "y_hat_cv_LOO = cvp(regr, X_train, y_train, cv=2746) #this thing is taking so long\n",
    "\n",
    "my_r2_plot(y_train,y_hat_cv_LOO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_hat_cv,y_hat_cv_LOO))\n",
    "#we just use cv=80 instead of leave-one-one since they have a large correlation (0.999937568174697) anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6:\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#recall that\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_zip(X_train.columns,regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means that:\n",
    "- 1 additional bathroom increases the parcel value by 1326.20\n",
    "- 1 additional bedroom decreass the parcel value by 129300.41\n",
    "- 1 additional unit of finished are increases the parcel value by 460.58\n",
    "- and so on and so forth. \n",
    "\n",
    "Some have counter intuitive signs but we will ignore this for now as this is not our final model.\n",
    "Further, the one with the highest magnitude is number of bedrooms, with an absolute value of 129,300, which is also counterintuitive. But we will ignore this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Prepare code to run and check performance of you model using a new input data with same exact format\n",
    "y_hat_test = regr.predict(X_test) #we produce predictions from our fitted model based on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Features and Dummy Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try the model with polynomial features and dummies.\n",
    "\n",
    "I carefully separate the variables that I will use for plf and dummies.\n",
    "\n",
    "I separated them because for me, it does not make much sense to generate polynomial features of dummy variables since this will just generate more duplicate columns (with terms such as 1^2, 1^3, 0^2, 0^3 whihch does not make any sense, and would just make perfectly collineary columns) and would make the computation time too long.\n",
    "\n",
    "Note to self: careful to make the variable names not confusing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let f be a shorthand for feature (to mark without making the var names too long) and let d be a shorthand for dummies.\n",
    "\n",
    "I thought of not getting the polynomial features of year and tax year because for me, it makes little sense to get variable transformations of these, therefore I will not include them in my feautures dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dummy Variable Generation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check first the number of unique variables that we plan to have dummies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential = ['heatingtype', 'citycode', 'countycode', 'neighborhoodcode', 'regioncode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in potential:\n",
    "    print(len(pd.unique(train[i])))\n",
    "\n",
    "for i in potential:\n",
    "    print(len(pd.unique(test[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_unique_print(c,d):\n",
    "    for i in d:\n",
    "        print(len(pd.unique(c[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len_unique_print(train,potential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are too many unique values for neighborhoodcode and regioncode for both the train and test datasets\n",
    "We will also create dummy for regioncode and neighborhoodcode but we will impose additional constraints on which values we are making dummies on. I only want to create dummies for neighborhoods and regions that occurs frequently in our dataset. Hence, I will use a cut-off.\n",
    "\n",
    "We prepare our neigborhoodcode and regioncode columns so that they won't be too many when we generate dummies. I will do the one for neighborhoods first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, it is also important to make sure that the dummies in the train dataset will be the same as in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_stats = X_train['neighborhoodcode'].value_counts(ascending=False) #we count the number of unique values, sort them from highest to lowest, store the results to neighborhood_stats\n",
    "neighborhood_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_stats.values.sum() #checking if it would sum to total number of obs, and it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neighborhood_stats[neighborhood_stats>100]) #counting neighborhoodcodes with more than 100 obs\n",
    "#I've decided that I am okay with have 30+1-1 dummies (neighborhoodcodes with more than 100 obs + others - drop_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neighborhood_stats[neighborhood_stats<=100]) #there are 307 neighborhoods with less than 100 obs\n",
    "#all of these neighborhoods will be identified as others later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_stats_less_than_100 = neighborhood_stats[neighborhood_stats<=100] #we store the neighborhoods will less than 100 obs\n",
    "neighborhood_stats_less_than_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_stats_less_than_100.shape #contains one column of the number of obs of neighborhoods with less than 100 obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.neighborhoodcode.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.neighborhoodcode = X_train.neighborhoodcode.apply(lambda x: 'other' if x in neighborhood_stats_less_than_100 else x) #we are replacing the values with 'other' for those neighborhoods with less than 100 obs\n",
    "len(X_train.neighborhoodcode.unique()) #now, we only have 31 unique neighborhoodcodes instead of 337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, doing the same for regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_stats = X_train['regioncode'].value_counts(ascending=False) #we count the number of unique values, sort them from highest to lowest, store the results to region_stats\n",
    "region_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_stats.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(region_stats[region_stats>100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(region_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(region_stats[region_stats<=100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_stats_less_than_100 = region_stats[region_stats<=100]\n",
    "region_stats_less_than_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.regioncode.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.regioncode = X_train.regioncode.apply(lambda x: 'other' if x in region_stats_less_than_100 else x) # #we are replacing the values with 'other' for those regions with less than 100 obs\n",
    "len(X_train.regioncode.unique()) #now, we only have 43 unique regioncodes instead of 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same encoding for the test dataset.\n",
    "\n",
    "Note that we only do the last step because we want the test dataset to have the same dummies as the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.neighborhoodcode = X_test.neighborhoodcode.apply(lambda x: 'other' if x in neighborhood_stats_less_than_100 else x)\n",
    "len(X_test.neighborhoodcode.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.regioncode = X_test.regioncode.apply(lambda x: 'other' if x in region_stats_less_than_100 else x)\n",
    "len(X_test.regioncode.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, there is more than one neighborhoodcode and one regioncode in the test data compared to the train. I will just drop the extra dummy from the test dataset later while doing the data alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) #I just checked all the obs of the test to make sure there's nothing weird going on.\n",
    "print(X_test['neighborhoodcode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally generate the dummies. Note that I will be dropping the first columns using drop_first=True to avoid the dummy variable trap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the dummies for the train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d = X_train[potential]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d = pd.get_dummies(X_train_d, columns=potential,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the dummies for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_d = X_test[potential]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_d = pd.get_dummies(X_test_d, columns=potential,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sub-DataFrame with dummies for the train dataset have 99 columns, while for the test, there are 105. This has occured because when we stored the neighborhoodcodes and regioncodes with less than 100 observations, some codes in neighborhoodcodes and regioncodes that should've been identified as other was for the test was not identified. This is just a small difference so we will just solve this by dropping the extra dummies while we align later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we store the variable names of the dummy because this will be important later on when we are matching the number of columns in the train and test dataset\n",
    "#we need to store the column names of these dummies because it will be gone once we convert it to numpy array or concatenate it later\n",
    "X_train_d_columns = list(X_train_d.columns)\n",
    "X_test_d_columns = list(X_test_d.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#just checking if the column names were stored properly\n",
    "X_test_d_columns\n",
    "X_train_d_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train_d_columns))\n",
    "print(len(X_test_d_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I store the year and tax year variables as we will drop them later when we generate polynomail features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_year = X_train[['year', 'taxyear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_year = X_test[['year', 'taxyear']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating Polynomial Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a sub-DataFrame that will contain the features that I want to transform using polynomial features.\n",
    "\n",
    "I do this for both the train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f = X_train.drop(['heatingtype', 'citycode', 'countycode', 'neighborhoodcode', 'regioncode', 'year', 'taxyear'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same for the test data\n",
    "X_test_f = X_test.drop(['heatingtype', 'citycode', 'countycode', 'neighborhoodcode', 'regioncode', 'year', 'taxyear'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_f.shape)\n",
    "print(X_test_f.shape)\n",
    "\n",
    "#we will be using the dataframes above later to generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally generate the polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures as plf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, let's try plf of order 2\n",
    "#note that I'm not doing instantiate and fit in one go because when I tried it, there are having errors when I use the .get_feature_names method\n",
    "order = 2\n",
    "poly = plf(order)\n",
    "\n",
    "phi_train = poly.fit_transform(X_train_f)\n",
    "phi_test = poly.fit_transform(X_test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I have tried having polynomial features of order 5, 4, and 3 respectively. But I couldn't fit the lasso without taking so much time, so I resort to 2. \n",
    "\n",
    "Further, for this dataset, I see no reason why there will be cubic and high order relationships for the features. But if I had more computing power, having higher-ordered features will be interesting if it wouldn't be zeroed out by an optimized Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we compare the number of columns in before and after the plf (9 vs. 55)\n",
    "\n",
    "print(X_train_f.shape)\n",
    "print(X_test_f.shape)\n",
    "\n",
    "print(phi_train.shape)\n",
    "print(phi_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dimensions of the features is 55 since polynomial features not only adds powers of each feature but also the interactions between them. Therefore, if we initial have 9 features, we do not just expect 9*2+1 in our new matrix phi_train.\n",
    "\n",
    "The formula for calculating the number of the polynomial features is N(n,d)=C(n+d,d) where n is the number of the features, d is the degree of the polynomial, C is binomial coefficient(combination). So in this case, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.factorial(11)/(math.factorial(11-2)*math.factorial(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_features(n,d):\n",
    "    return math.factorial(n+d)/(math.factorial(n+d-d)*math.factorial(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_features(9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(phi_train).head()\n",
    "#we observe that the features we have transformed trhu plf have no column names\n",
    "#but we can get their names (as shown in the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's store the names of these features because it's important for the test and train dataset alignment later\n",
    "import numpy as np\n",
    "phi_train_columns = np.array(poly.get_feature_names(X_train_f.columns))\n",
    "phi_test_columns = np.array(poly.get_feature_names(X_test_f.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_train_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's concatenate the columns names for both the dummies and the features, as well as the years columns\n",
    "final_X_train_columns = np.concatenate([phi_train_columns, X_train_year.columns, X_train_d_columns])\n",
    "final_X_test_columns = np.concatenate([phi_test_columns, X_test_year.columns, X_test_d_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_X_train_columns.shape)\n",
    "print(final_X_test_columns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(final_X_train_columns)\n",
    "print(final_X_test_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the generated dummies, years, and generated polynomial features to have our almost final datasets. (Almost final because we will have to do rescaling later). Note that this dataset contains a constant term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_train = np.concatenate((phi_train, X_train_year, X_train_d), axis=1)\n",
    "final_X_test = np.concatenate((phi_test, X_test_year, X_test_d), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_train = pd.DataFrame(final_X_train)\n",
    "final_X_test = pd.DataFrame(final_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_train.columns = final_X_train_columns\n",
    "final_X_test.columns = final_X_test_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_X_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_X_test).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_X_train.shape)\n",
    "print(final_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We align the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_train = pd.DataFrame(final_X_train)\n",
    "final_X_test = pd.DataFrame(final_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing columns in the training test\n",
    "missing_cols = set( final_X_train.columns ) - set( final_X_test.columns )\n",
    "# Add a missing column in test set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    final_X_test[c] = 0\n",
    "# Ensure the order of column in the test set is in the same order than in train set\n",
    "final_X_test= final_X_test[final_X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_X_train.shape)\n",
    "print(final_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the second model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression using transformed features (no scaling yet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regr  = LinearRegression(fit_intercept=False) ## fit_intercept = False as we already have it in phi earlier\n",
    "regr.fit(final_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_final = regr.predict(final_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_r2_plot(y_train, y_hat_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that 0.513 is better than 0.478 in-sample correlation before the variable transformations (polynomial features and dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU0jANEHf-cq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##producing cross-validated predictions\n",
    "y_hat_cv_final = cvp(regr, final_X_train, y_train, cv=80) # doing cv=80 because LOO takes too long\n",
    "\n",
    "my_r2_plot(y_train, y_hat_cv_final)\n",
    "\n",
    "# the value of R-squared is 0.360, there is some correlation between the y_train but not strong\n",
    "#therefore go back to previous steps and optimize paramaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that the r-squared with cross-validated predictions is much lower than the usual in-sample r-squared (0.524 < 0.557). In other words, the performance of the model decreased drastically.\n",
    "\n",
    "Notice also that the difference between the in-sample and cvp of our original dataset is smaller compared to the one with transform dataset  (0.485-0.489 = **0.06**) vs (0.513 - 0.383 =  **0.130**)\n",
    "\n",
    "Therefore, if we transform are variables without penalizing (just the usual regression or alpha = 1, our model would be more inaccuarate compared to when there was no feature transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6:\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = regr.predict(final_X_test) #we produce predictions for our test dataset from our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Step 8:\n",
    "#test_predictions_submit = pd.DataFrame({\"lotid\": test_original[\"lotid\"], \"parcelvalue\": y_hat_test})\n",
    "#test_predictions_submit.to_csv(\"test_predictions_submit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we've seen that the score for this model perhaps could be very much improved so we use now lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the intercept before standardization\n",
    "final_X_train = final_X_train.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_test = final_X_test.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the standardization that Jack used in class. But this returns the error \"Dataset may contain too large values. You may need to prescale your features.\" So I will use StandardScaler instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation of input is critical: We will use sklearn to do this\n",
    "\n",
    "# generic lasso regression object\n",
    "#from sklearn.preprocessing import scale as scl\n",
    "#scaled_final_X_train = scl(final_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##standardization of train data before lasso\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# Get column names first\n",
    "names = final_X_train.columns\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_final_X_train = scaler.fit_transform(final_X_train)\n",
    "scaled_final_X_train = pd.DataFrame(scaled_final_X_train, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##standardization of test data before lasso\n",
    "\n",
    "# Get column names first\n",
    "names = final_X_test.columns\n",
    "\n",
    "# Fit your data on the scaler object\n",
    "#We do not need to fit the objects again. \n",
    "#For sc, we want to keep the method we used to fit X_train_poly. \n",
    "#This means that the test data will not be perfectly standardised, and that is fine. \n",
    "#So instead of fit_transform, we use transform.\n",
    "scaled_final_X_test = scaler.transform(final_X_test)\n",
    "scaled_final_X_test = pd.DataFrame(scaled_final_X_test, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final_X_test.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that the dummy columns are not with 0 and 1's anymore but they still contain binary values, so that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(scaled_final_X_train))\n",
    "print(type(scaled_final_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I change the type/class to a numpy array because that would be faster\n",
    "scaled_final_X_train_np = np.array(scaled_final_X_train)\n",
    "scaled_final_X_test_np = np.array(scaled_final_X_test)\n",
    "y_train_np = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(a):\n",
    "    for i in a:\n",
    "        print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shape(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the third model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the proceeding models use Lasso, but with different ways how to find and set the hyperparameter alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso using transformed features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use LassoLarsIC (whose results are based on AIC/BIC criteria) for faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lasso using LassoLarsIC, using the Akaike Information Criterion (AIC)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_lasso = linear_model.LassoLarsIC(criterion=\"aic\", normalize=False, max_iter = 100000)\n",
    "regr_lasso.fit(scaled_final_X_train_np, y_train_np)\n",
    "alpha_regr_lasso = regr_lasso.alpha_\n",
    "print(alpha_regr_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do it with a graph to better visualize how the model was selected through the AIC. We also plot it agains BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# This is to avoid division by zero while doing np.log10\n",
    "EPSILON = 1e-4\n",
    "\n",
    "# LassoLarsIC: least angle regression with BIC/AIC criterion\n",
    "\n",
    "model_bic = linear_model.LassoLarsIC(criterion=\"bic\", normalize=False)\n",
    "t1 = time.time()\n",
    "model_bic.fit(scaled_final_X_train_np, y_train_np)\n",
    "t_bic = time.time() - t1\n",
    "alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "model_aic = linear_model.LassoLarsIC(criterion=\"aic\", normalize=False)\n",
    "model_aic.fit(scaled_final_X_train_np, y_train_np)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "\n",
    "def plot_ic_criterion(model, name, color):\n",
    "    criterion_ = model.criterion_\n",
    "    plt.semilogx(\n",
    "        model.alphas_ + EPSILON,\n",
    "        criterion_,\n",
    "        \"--\",\n",
    "        color=color,\n",
    "        linewidth=3,\n",
    "        label=\"%s criterion\" % name,\n",
    "    )\n",
    "    plt.axvline(\n",
    "        model.alpha_ + EPSILON,\n",
    "        color=color,\n",
    "        linewidth=3,\n",
    "        label=\"alpha: %s estimate\" % name,\n",
    "    )\n",
    "    plt.xlabel(r\"$\\alpha$\")\n",
    "    plt.ylabel(\"criterion\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plot_ic_criterion(model_aic, \"AIC\", \"b\")\n",
    "plot_ic_criterion(model_bic, \"BIC\", \"r\")\n",
    "plt.legend()\n",
    "plt.title(\"Information-criterion for model selection (training time %.3fs)\" % t_bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alpha_bic_)\n",
    "print(alpha_aic_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regr_lasso = linear_model.LassoLarsIC(criterion='aic', fit_intercept=True, max_iter=100000, normalize=False) #we've dropped the constant before scaling so we will fit the intercept\n",
    "regr_lasso.fit(scaled_final_X_train,y_train)\n",
    "\n",
    "#352 iterations, alpha=1.441e-06, previous alpha=1.333e-06, with an active set of 151 regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr_lasso.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "print(regr_lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that some coefficients are zeroed out. This means that our model done feature selection and parameter tuning at the same time. Let's check how many features have been retained (and zeroed out using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total coefficiets:\", len(regr_lasso.coef_))\n",
    "print(\"Non-zero coefficiets:\", np.count_nonzero(regr_lasso.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now 136 non-zero coefficients. This means that 18 features have been zeroed out by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features have been zeroed out by lasso? Examples are 'numfireplace', 'roomnum', 'numbathnumbedroom', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_zip(final_X_train.columns, regr_lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = regr_lasso.predict(scaled_final_X_train) #we produce predictions from our fitted model based on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IN-SAMPLE\n",
    "my_r2_plot(y_train, y_hat_train)\n",
    "\n",
    "#we observe that this model has higer correlation / higher explanatory power than the OLS ( 0.573 vs. 0.485), \n",
    "#so this a good sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##producing cross-validated predictions\n",
    "y_hat_cv = cvp(regr_lasso, scaled_final_X_train, y_train, cv=80) # doing cv=80 because LOO takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_r2_plot(y_train, y_hat_cv)\n",
    "\n",
    "\n",
    "#the cross-validated predictions worse, however (0.480 vs 0.421). The reduction in score is also higher.\n",
    "#but still, our results aren't that bad!!\n",
    "#however, when I submitted my predictions to Kaggle, it returned a really high NMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = regr_lasso.predict(scaled_final_X_test_np) \n",
    "#the submitted predictions to kaggle returned a high negative mean square error: 841082.34786\n",
    "#why? this is my worst performing model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just some exploration**\n",
    "\n",
    "Now checking again using just OLS, but now with the scaled data. This is because I'm wondering why my Lasso perfomed worse than the usual Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regr = LinearRegression(fit_intercept=True)\n",
    "regr.fit(scaled_final_X_train_np,y_train_np)\n",
    "regr.score(scaled_final_X_train_np,y_train_np)\n",
    "\n",
    "#the result is quite good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##producing cross-validated predictions\n",
    "y_hat_cv = cvp(regr, scaled_final_X_train, y_train, cv=40) # doing cv=40 because LOO takes too long\n",
    "\n",
    "regr.score(scaled_final_X_train_np,y_hat_cv)\n",
    "\n",
    "#Why is the usual regression doing really bad using scaled data under cross-validated predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the fourth model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso with some random alpha I've set. Our previous model used alpha=1.57. I'll use a smaller alpha this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_lasso = linear_model.Lasso(random_state = 0, max_iter=500000, alpha=0.001, tol=0.1, fit_intercept=True) \n",
    "#random state is setting seed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_lasso.fit(scaled_final_X_train_np, y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "print(regr_lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total coefficiets:\", len(regr_lasso.coef_))\n",
    "print(\"Non-zero coefficiets:\", np.count_nonzero(regr_lasso.coef_))\n",
    "#none of the coefficients have been zeroed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN-SAMPLE ##\n",
    "y_hat = regr_lasso.predict(scaled_final_X_train)\n",
    "#print(y_hat.shape)\n",
    "\n",
    "my_r2_plot(y_train, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##producing cross-validated predictions\n",
    "y_hat_cv = cvp(regr_lasso, scaled_final_X_train_np, y_train_np, cv=40) # doing cv=40 because LOO takes too long\n",
    "\n",
    "print(r2_score(y_train,y_hat_cv)) #equals to 0.42229123280112824\n",
    "print(regr_lasso.score(scaled_final_X_train_np,y_hat_cv)) # equals to #equals to 0.977778279489165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we try on test dataset\n",
    "y_hat_test = regr_lasso.predict(scaled_final_X_test)\n",
    "\n",
    "#kaggle returned an RMSE of 841082.34791"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the fifth model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso again, with grid search of alphas this time. We include 1.57 and 0.001, the alphas we've used before.\n",
    "\n",
    "I've tried many times, but oftentimes, the model does not converge, unless I set tol=1. However, upon checking the coefficients, I was not able to zero out anything. So I will change tol to a smaller value.However, I've run the search for 6 hours and it still has not finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lasso = linear_model.Lasso(random_state = 0, max_iter=5000000, tol=1)\n",
    "\n",
    "\n",
    "alphas = np.array([0.000007, 0.00008,0.00012,0.0005, 0.001,1.5736681638814658]) #we include the alpha we got from the AIC\n",
    "alphas_long = np.array([0.000007, 0.00002, 0.00005,0.00008,0.00012,0.0002,0.0003,0.0005,0.0006,0.002])\n",
    "\n",
    "tuned_parameters = [{'alpha': alphas}] ## dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scorer to evaluate performance\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer \n",
    "\n",
    "## ALWAYS read carefully documentation. copying here from make_scorer\n",
    "## greater_is_better : boolean, default=True\n",
    "# \"Whether score_func is a score function (default), meaning high is \n",
    "# good, or a loss function, meaning low is good. \n",
    "# In the latter case, the scorer object will sign-flip \n",
    "# the outcome of the score_func.\n",
    "mse = make_scorer(mean_squared_error,greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_folds = 10 \n",
    "\n",
    "clf = GridSearchCV(lasso, tuned_parameters, scoring = mse, \n",
    "                   cv=n_folds, refit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(scaled_final_X_train, y_train)\n",
    "\n",
    "\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "scores_std = clf.cv_results_['std_test_score']\n",
    "std_error = scores_std / np.sqrt(n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best param\n",
    "clf.best_params_\n",
    "\n",
    "#the search identified the smallest alpha from the leastas the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "regr_lasso = Lasso(alpha=0.000007, random_state = 0, max_iter=3000000, tol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_lasso.fit(scaled_final_X_train_np,y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "print(regr_lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## IN-SAMPLE ##\n",
    "y_hat = regr_lasso.predict(scaled_final_X_train_np)\n",
    "#print(y_hat.shape)\n",
    "\n",
    "my_r2_plot(y_train, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = regr_lasso.predict(scaled_final_X_test_np)\n",
    "\n",
    "#RMSE from Kaggle = 647076.20247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some more exploration using Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare cross validation scores from Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "for Model in [Ridge, Lasso]:\n",
    "    model = Model()\n",
    "    print('%s: %s' % (Model.__name__,\n",
    "                      cross_val_score(model, scaled_final_X_train_np, y_train_np).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, -1, 30)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "for Model in [Lasso, Ridge]:\n",
    "    scores = [cross_val_score(Model(alpha), scaled_final_X_train_np, y_train_np, cv=3).mean()\n",
    "            for alpha in alphas]\n",
    "    plt.plot(alphas, scores, label=Model.__name__)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('cross validation score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, I conclude that the optimal alpha is something very small (close to 0). This is in line with the results of our GridSeachCV but not with LassoLarsIC using AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building sixth model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try fitting the model using Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = Ridge(alpha=0.0000001)\n",
    "clf.fit(scaled_final_X_train_np, y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_ridge = clf.predict(scaled_final_X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_train_np, y_hat_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_ridge_cv = cvp(clf, scaled_final_X_train_np, y_train_np, cv=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_train_np, y_hat_ridge_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_ridge_test = clf.predict(scaled_final_X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final_X_test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_final_X_train_np.shape)\n",
    "print(scaled_final_X_test_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Other codes that I tried to do but took too long***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearcCV is taking too long, better to use RandomizedGridSearchInstead\n",
    "\n",
    "#small\n",
    "#cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "#lasso_alphas = np.array([0.000007, 0.00005,0.00012, 0.00025,0.0005,0.002])\n",
    "#lasso = Lasso(random_state=0,max_iter=100000)\n",
    "#grid = dict()\n",
    "#grid['alpha'] = lasso_alphas\n",
    "#gscv = GridSearchCV( \\\n",
    "    #lasso, grid, scoring='neg_root_mean_squared_error', \\\n",
    "    #cv=50, n_jobs=-1)\n",
    "#results = gscv.fit(scaled_final_X_train_df, y_train)\n",
    "\n",
    "#print('MAE: %.5f' % results.best_score_)\n",
    "#print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big\n",
    "#cv = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "#lasso_alphas = np.array([0.000007, 0.00002, 0.00004, 0.00005,0.00008,0.0001,0.00012, 0.00015,0.0002,0.00025,0.0003,0.0004,0.0005,0.0006,0.0007,0.002])\n",
    "#lasso = Lasso(random_state=0, max_iter = 1000000, tol=0.01)\n",
    "#grid = dict()\n",
    "#grid['alpha'] = lasso_alphas\n",
    "#gscv = GridSearchCV( \\\n",
    "    #lasso, grid, scoring='neg_root_mean_squared_error', \\\n",
    "    #cv=50, n_jobs=-1)\n",
    "#results = gscv.fit(scaled_final_X_train_new_df, y_train)\n",
    "\n",
    "#print('MAE: %.5f' % results.best_score_)\n",
    "#print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I've tried doing the lasso using the following parameters but it returned a worse fit that if only I did a simple linear regression\n",
    "#further, there are some values that are negative\n",
    "#let's try to force the coefficients to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso\n",
    "#from sklearn.linear_model import Lasso\n",
    "#alpha is what was lambda in our notation\n",
    "#i'll change the max_iter recommended by the class notes to a smaller one since 1000000 it's taking too long \n",
    "#for my computer to fit the data\n",
    "#regr_lasso = Lasso(alpha=0.0001, fit_intercept=False,warm_start=True,max_iter=100000)\n",
    "#regr_lasso = Lasso(alpha=50, fit_intercept=False,warm_start=False,max_iter=50000, tol=0.1)\n",
    "#regr_lasso = Lasso(alpha=5, fit_intercept=False,warm_start=False,max_iter=100000, tol=0.1)\n",
    "#regr_lasso = Lasso(alpha=1, fit_intercept=False,warm_start=True,max_iter=10000)\n",
    "\n",
    "regr_lasso = Lasso(alpha=1, fit_intercept=False, warm_start=True,max_iter=1000000, tol=0.1, positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regr_lasso.fit(scaled_final_X_train_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(regr_lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoV3imfef-cq"
   },
   "outputs": [],
   "source": [
    "# Step 6: Report variable impact\n",
    "\n",
    "# Report of the coefficients every after model fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yj97T2df-cq"
   },
   "outputs": [],
   "source": [
    "# Step 7: Prepare code to run and check performance of you model using a new input data with same exact format\n",
    "\n",
    "#other step 7's are above (after the in-sample and cross validation predict)\n",
    "y_hat_test = regr_lasso.predict(scaled_final_X_test_new_df) #we produce predictions from our fitted model based on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Predictions Submissions\n",
    "\n",
    "Once you have produced testset predictions you can submit these to <i> kaggle </i> in order to see how your model performs. \n",
    "\n",
    "The following code provides an example of generating a <i> .csv </i> file to submit to kaggle\n",
    "1) create a pandas dataframe with two columns, one with the test set \"lotid\"'s and the other with your predicted \"parcelvalue\" for that observation\n",
    "\n",
    "2) use the <i> .to_csv </i> pandas method to create a csv file. The <i> index = False </i> is important to ensure the <i> .csv </i> is in the format kaggle expects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Produce .csv for kaggle testing \n",
    "test_predictions_submit = pd.DataFrame({\"lotid\": test_original[\"lotid\"], \"parcelvalue\": y_hat_test})\n",
    "test_predictions_submit.to_csv(\"test_predictions_submit.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "linear_models_project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "430.852px",
    "left": "59px",
    "top": "110.322px",
    "width": "195.994px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
